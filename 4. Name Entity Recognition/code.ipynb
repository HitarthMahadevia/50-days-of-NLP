{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "### **Named Entity Recognition (NER)**\n",
    "Named Entity Recognition (NER) is a critical task in Natural Language Processing (NLP) that involves identifying and categorizing named entities in text, such as names of people, organizations, and locations. \n",
    "\n",
    "Here’s a more detailed breakdown of how we can approach NER:\n",
    "\n",
    "1. **Tokenization and POS Tagging:**\n",
    "\n",
    "    - Tokenization: This step involves breaking down the text into individual tokens or words. It’s the first step in preparing text for further analysis.\n",
    "\n",
    "    - POS Tagging: Part-of-Speech (POS) tagging assigns a grammatical category (tag) to each token, such as noun, verb, adjective, etc. This information is crucial for identifying named entities because entities often follow specific patterns of POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: ['Apple', 'is', 'planning', 'to', 'build', 'a', 'new', 'store', 'in', 'London', '.']\n",
      "POS Tags: [('Apple', 'NNP'), ('is', 'VBZ'), ('planning', 'VBG'), ('to', 'TO'), ('build', 'VB'), ('a', 'DT'), ('new', 'JJ'), ('store', 'NN'), ('in', 'IN'), ('London', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample text for demonstration\n",
    "text = \"Apple is planning to build a new store in London.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(\"Tokenized Text:\", tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chunking:**\n",
    "\n",
    "Chunking is the process of grouping tokens into chunks, often based on specific patterns of POS tags. In NER, chunking helps to identify sequences of tokens that together form named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Result:\n",
      "(S\n",
      "  (NE Apple/NNP)\n",
      "  is/VBZ\n",
      "  planning/VBG\n",
      "  to/TO\n",
      "  build/VB\n",
      "  a/DT\n",
      "  new/JJ\n",
      "  store/NN\n",
      "  in/IN\n",
      "  (NE London/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "# Define grammar for chunking\n",
    "grammar = r\"\"\"\n",
    "    NE: {<NNP>+}    # Chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "\n",
    "# Create chunk parser with defined grammar\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "\n",
    "# Apply chunking to POS tagged tokens\n",
    "chunked_result = chunk_parser.parse(pos_tags)\n",
    "\n",
    "print(\"Chunked Result:\")\n",
    "print(chunked_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explanation of the Grammar (regex pattern):\n",
    "\n",
    "    `NE: {<NNP>+}`: This rule specifies that we want to chunk sequences (+) of proper nouns (<NNP>) into a single chunk labeled as NE.\n",
    "\n",
    "- Creating a Chunk Parser:\n",
    "\n",
    "    `RegexpParser(grammar)`: This initializes a chunk parser using the grammar defined by the regex pattern.\n",
    "\n",
    "- Applying Chunking:\n",
    "\n",
    "    `chunk_parser.parse(pos_tags)`: This applies the chunking rules defined by chunk_parser to the POS tagged tokens (pos_tags) obtained from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Named Entity Recognition with NLTK or SpaCy:**\n",
    "\n",
    "- NLTK: Provides tools for tokenization, POS tagging, and basic chunking. It's versatile and suitable for learning and prototyping.\n",
    "\n",
    "- SpaCy: A more modern NLP library that offers efficient tokenization, POS tagging, and built-in NER capabilities. It provides pre-trained models for various languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.0)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hitar\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/12.1 MB 9.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 16.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.5/12.1 MB 25.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.6/12.1 MB 26.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.8/12.1 MB 24.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.5/12.1 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.1 MB 28.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 31.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 29.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.0/182.0 kB ? eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB ? eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 479.7/479.7 kB 15.1 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.7/1.5 MB 23.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 18.8 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.2/47.2 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB ? eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.7/6.6 MB 45.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 20.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.5/6.6 MB 20.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.4/6.6 MB 20.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 20.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.3/6.6 MB 19.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 19.2 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.1/5.4 MB 24.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.1/5.4 MB 22.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.2/5.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.2/5.4 MB 22.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.3/5.4 MB 22.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 21.5 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.6/152.6 kB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 shellingham-1.5.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "London GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy's English NLP pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: ['In', 'the', 'bustling', 'city', 'of', 'New', 'York', ',', 'Apple', 'Inc.', 'is', 'set', 'to', 'unveil', 'its', 'latest', 'iPhone', 'model', 'at', 'a', 'highly', 'anticipated', 'event', '.', 'The', 'European', 'Union', '(', 'EU', ')', 'is', 'facing', 'economic', 'challenges', 'amid', 'Brexit', 'negotiations', ',', 'while', 'NATO', 'continues', 'to', 'strengthen', 'its', 'defense', 'initiatives', 'in', 'response', 'to', 'global', 'security', 'threats', '.', 'The', 'Amazon', 'rainforest', ',', 'a', 'vital', 'ecosystem', 'in', 'South', 'America', ',', 'is', 'under', 'threat', 'from', 'deforestation', ',', 'prompting', 'environmental', 'activists', 'to', 'call', 'for', 'urgent', 'conservation', 'efforts', '.', 'Meanwhile', ',', 'Harvard', 'University', 'remains', 'a', 'beacon', 'of', 'academic', 'excellence', ',', 'attracting', 'students', 'and', 'researchers', 'from', 'around', 'the', 'world', 'to', 'its', 'historic', 'campus', 'in', 'Cambridge', '.']\n",
      "POS Tags: [('In', 'IN'), ('the', 'DT'), ('bustling', 'JJ'), ('city', 'NN'), ('of', 'IN'), ('New', 'NNP'), ('York', 'NNP'), (',', ','), ('Apple', 'NNP'), ('Inc.', 'NNP'), ('is', 'VBZ'), ('set', 'VBN'), ('to', 'TO'), ('unveil', 'VB'), ('its', 'PRP$'), ('latest', 'JJS'), ('iPhone', 'NN'), ('model', 'NN'), ('at', 'IN'), ('a', 'DT'), ('highly', 'RB'), ('anticipated', 'JJ'), ('event', 'NN'), ('.', '.'), ('The', 'DT'), ('European', 'NNP'), ('Union', 'NNP'), ('(', '('), ('EU', 'NNP'), (')', ')'), ('is', 'VBZ'), ('facing', 'VBG'), ('economic', 'JJ'), ('challenges', 'NNS'), ('amid', 'IN'), ('Brexit', 'NNP'), ('negotiations', 'NNS'), (',', ','), ('while', 'IN'), ('NATO', 'NNP'), ('continues', 'VBZ'), ('to', 'TO'), ('strengthen', 'VB'), ('its', 'PRP$'), ('defense', 'NN'), ('initiatives', 'NNS'), ('in', 'IN'), ('response', 'NN'), ('to', 'TO'), ('global', 'JJ'), ('security', 'NN'), ('threats', 'NNS'), ('.', '.'), ('The', 'DT'), ('Amazon', 'NNP'), ('rainforest', 'NN'), (',', ','), ('a', 'DT'), ('vital', 'JJ'), ('ecosystem', 'NN'), ('in', 'IN'), ('South', 'NNP'), ('America', 'NNP'), (',', ','), ('is', 'VBZ'), ('under', 'IN'), ('threat', 'NN'), ('from', 'IN'), ('deforestation', 'NN'), (',', ','), ('prompting', 'VBG'), ('environmental', 'JJ'), ('activists', 'NNS'), ('to', 'TO'), ('call', 'VB'), ('for', 'IN'), ('urgent', 'JJ'), ('conservation', 'NN'), ('efforts', 'NNS'), ('.', '.'), ('Meanwhile', 'RB'), (',', ','), ('Harvard', 'NNP'), ('University', 'NNP'), ('remains', 'VBZ'), ('a', 'DT'), ('beacon', 'NN'), ('of', 'IN'), ('academic', 'JJ'), ('excellence', 'NN'), (',', ','), ('attracting', 'VBG'), ('students', 'NNS'), ('and', 'CC'), ('researchers', 'NNS'), ('from', 'IN'), ('around', 'IN'), ('the', 'DT'), ('world', 'NN'), ('to', 'TO'), ('its', 'PRP$'), ('historic', 'JJ'), ('campus', 'NN'), ('in', 'IN'), ('Cambridge', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Sample text for demonstration\n",
    "text = \"In the bustling city of New York, Apple Inc. is set to unveil its latest iPhone model at a highly anticipated event. The European Union (EU) is facing economic challenges amid Brexit negotiations, while NATO continues to strengthen its defense initiatives in response to global security threats. The Amazon rainforest, a vital ecosystem in South America, is under threat from deforestation, prompting environmental activists to call for urgent conservation efforts. Meanwhile, Harvard University remains a beacon of academic excellence, attracting students and researchers from around the world to its historic campus in Cambridge.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(\"Tokenized Text:\", tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York GPE\n",
      "Apple Inc. ORG\n",
      "The European Union ORG\n",
      "EU ORG\n",
      "Brexit PERSON\n",
      "NATO ORG\n",
      "Amazon ORG\n",
      "South America LOC\n",
      "Harvard University ORG\n",
      "Cambridge GPE\n"
     ]
    }
   ],
   "source": [
    "# Load SpaCy's English NLP pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
