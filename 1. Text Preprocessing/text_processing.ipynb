{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Flow of the notebook:**\n",
    "    In natrual language processing, the text procession involves 4 major steps. This steps are as listed below:\n",
    "        1. Tokenization\n",
    "        2. Stop Word Removal\n",
    "        3. Stemming\n",
    "        4. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets start with the first step.\n",
    "\n",
    "### **Step 01: Tokenization**\n",
    "\n",
    "**Understanding Tokenization**\n",
    "\n",
    "Tokenization is the process of breaking down a piece of text into smaller units called tokens. These tokens can be words, sentences, or even subwords. Tokenization is the first step in text preprocessing and is crucial for preparing text data for further processing by NLP algorithms.\n",
    "\n",
    "**Types of Tokenization**\n",
    "\n",
    "**1. Word Tokenization**\n",
    "\n",
    "This involves splitting a text into individual words.\n",
    "For example, the sentence \"Hello there! How are you doing today?\" would be tokenized into [\"Hello\", \"there\", \"!\", \"How\", \"are\", \"you\", \"doing\", \"today\", \"?\"].\n",
    "\n",
    "**2.Sentence Tokenization**\n",
    "\n",
    "This involves splitting a text into sentences.\n",
    "For example, the paragraph \"Hello there! How are you doing today? I hope you're having a great day.\" would be tokenized into [\"Hello there!\", \"How are you doing today?\", \"I hope you're having a great day.\"].\n",
    "\n",
    "**Importance of Tokenization**\n",
    "\n",
    "**1. Data Preparation:** It converts text into a format that can be used for further processing and analysis.\n",
    "\n",
    "**2. Feature Extraction:** Tokens are used as features in various NLP tasks like text classification, sentiment analysis, etc.\n",
    "\n",
    "**3. Text Normalization:** Helps in cleaning and standardizing the text data.\n",
    "\n",
    "**Techniques for Tokenization**\n",
    "\n",
    "Different techniques and tools can be used for tokenization. We'll focus on a popular NLP library called NLTK (Natural Language Toolkit) in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered sentence is:  Hello Everyone. Welcome to the 50 days of NLP challenge.\n",
      "The words in the sentence are:  ['Hello', 'Everyone', '.', 'Welcome', 'to', 'the', '50', 'days', 'of', 'NLP', 'challenge', '.']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = input(\"Enter a sentence: \")\n",
    "result  = word_tokenize(text)\n",
    "\n",
    "print(\"Entered sentence is: \", text)\n",
    "print(\"The words in the sentence are: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence you have entered is:  Hello everyone. My name is Hitarth Mahadevia. Welcome to 50 days of NLP challenge. Hope you have a wonderful experience!\n",
      "The sentence tokenisation is:  ['Hello everyone.', 'My name is Hitarth Mahadevia.', 'Welcome to 50 days of NLP challenge.', 'Hope you have a wonderful experience!']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenisation\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = input(\"Enter a text: \")\n",
    "result = sent_tokenize(text)\n",
    "\n",
    "print(\"The sentence you have entered is: \",text)\n",
    "print(\"The sentence tokenisation is: \",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', '.']\n",
      "['It', 'focuses', 'on', 'building', 'systems', 'that', 'learn', 'from', 'data', '.']\n",
      "['NLP', 'is', 'a', 'key', 'application', 'of', 'machine', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "# combining both the techniques\n",
    "\n",
    "paragraph = input(\"Enter a paragraph: \")\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 02: Stop Word Removal**\n",
    "\n",
    "**Understanding Stop Words**\n",
    "\n",
    "Stop words are commonly used words in a language that are usually ignored during text processing because they don't carry significant meaning. These words include articles, prepositions, conjunctions, and some pronouns. \n",
    "\n",
    "Examples of stop words in English are \"is\", \"the\", \"in\", \"and\", \"to\", \"of\", etc.\n",
    "\n",
    "**Why Remove Stop Words?**\n",
    "\n",
    "1. Reduce Noise: Stop words add noise to the data, which can negatively impact the performance of NLP algorithms.\n",
    "\n",
    "2. Dimensionality Reduction: Removing stop words helps in reducing the dimensionality of the text data, making it easier to process.\n",
    "\n",
    "3. Focus on Important Words: By removing stop words, we can focus on the more meaningful words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hitar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without removing stop words:\n",
      "\n",
      "['Machine', 'learning', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', '.', 'It', 'focuses', 'on', 'building', 'systems', 'that', 'learn', 'from', 'data', '.', 'NLP', 'is', 'a', 'key', 'application', 'of', 'machine', 'learning', '.']\n",
      "\n",
      "Tokens after removing stop words:\n",
      "\n",
      "['Machine', 'learning', 'branch', 'artificial', 'intelligence', '.', 'focuses', 'building', 'systems', 'learn', 'data', '.', 'NLP', 'key', 'application', 'machine', 'learning', '.']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words using nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = input(\"Enter a text: \")\n",
    "result = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_result = [word for word in result if not word.lower() in stop_words]\n",
    "\n",
    "print(\"Tokens without removing stop words:\\n\")\n",
    "print(result)\n",
    "print(\"\\nTokens after removing stop words:\\n\")\n",
    "print(filtered_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Custom Stop word adding and removing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing many custom stop words:  ['showing', 'stop', '.']\n",
      "After removing only one custom stop word:  ['showing', 'additional', 'stop', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "# adding stop words\n",
    "custom_stop_words = set(stop_words)\n",
    "custom_stop_words.update([\"example\", \"additional\", \"words\"])\n",
    "\n",
    "\n",
    "text = \"This is an example showing additional stop words.\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in custom_stop_words]\n",
    "print(\"After removing many custom stop words: \",filtered_tokens)\n",
    "\n",
    "#adding a single stop_word\n",
    "custom_stop_words = set(stop_words)\n",
    "custom_stop_words.add(\"example\")\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in custom_stop_words]\n",
    "print(\"After removing only one custom stop word: \",filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop word removal using custom list:  ['Kanchan', 'Kumbdi', 'not', 'good', 'witch', '!']\n",
      "Stop word removal using original list:  ['Kanchan', 'Kumbdi', 'good', 'witch', '!']\n"
     ]
    }
   ],
   "source": [
    "#removing stop word\n",
    "custom_list = set(stopwords.words('english'))\n",
    "custom_list.discard('not')\n",
    "\n",
    "original_list = set(stopwords.words('english'))\n",
    "\n",
    "text = \"Kanchan Kumbdi is not a good witch!\"\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [token for token in tokens if token not in custom_list]\n",
    "print(\"Stop word removal using custom list: \",filtered_tokens)\n",
    "\n",
    "original_tokens = [token for token in tokens if token not in original_list]\n",
    "print(\"Stop word removal using original list: \",original_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 03: Stemming**\n",
    "\n",
    "**Understanding Stemming**\n",
    "\n",
    "Stemming is the process of reducing words to their root or base form. The root form of the word is called the \"stem.\" Stemming algorithms typically remove suffixes from words to convert them into their base form, which may or may not be a real word.\n",
    "\n",
    "**Why Use Stemming?**\n",
    "\n",
    "1. Dimensionality Reduction: Stemming helps in reducing the number of unique words in the text, which can simplify the data and reduce dimensionality.\n",
    "\n",
    "2. Improving Generalization: By converting different forms of a word into a single form, stemming helps algorithms generalize better, especially in tasks like text classification and sentiment analysis.\n",
    "\n",
    "\n",
    "**Common Stemming Algorithms**\n",
    "\n",
    "1. Porter Stemmer: One of the most widely used stemming algorithms.\n",
    "\n",
    "2. Lancaster Stemmer: A more aggressive stemming algorithm compared to the Porter Stemmer.\n",
    "\n",
    "3. Snowball Stemmer: An improved version of the Porter Stemmer that supports multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered Text: \n",
      " Machine learning is a branch of artificial intelligence. It focuses on building systems that learn from data. NLP is a key application of machine learning.\n",
      "\n",
      "Original Tokens: ['Machine', 'learning', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', '.', 'It', 'focuses', 'on', 'building', 'systems', 'that', 'learn', 'from', 'data', '.', 'NLP', 'is', 'a', 'key', 'application', 'of', 'machine', 'learning', '.']\n",
      "\n",
      "Stemmed Tokens: ['machin', 'learn', 'is', 'a', 'branch', 'of', 'artifici', 'intellig', '.', 'it', 'focus', 'on', 'build', 'system', 'that', 'learn', 'from', 'data', '.', 'nlp', 'is', 'a', 'key', 'applic', 'of', 'machin', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 01: Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Step 02: Input a text\n",
    "text = input(\"Enter a text: \")\n",
    "\n",
    "# Step 03: Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Step 04: Initialize the Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Step 05: Apply stemmer to each token\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Step 06: Display the tokens before and after stemming\n",
    "print(\"Entered Text: \\n\", text)\n",
    "print(\"\\nOriginal Tokens:\", tokens)\n",
    "print(\"\\nStemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Comparing different Stemmers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['Machine', 'learning', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', '.', 'It', 'focuses', 'on', 'building', 'systems', 'that', 'learn', 'from', 'data', '.', 'NLP', 'is', 'a', 'key', 'application', 'of', 'machine', 'learning', '.']\n",
      "Porter Stemmed Tokens: ['machin', 'learn', 'is', 'a', 'branch', 'of', 'artifici', 'intellig', '.', 'it', 'focus', 'on', 'build', 'system', 'that', 'learn', 'from', 'data', '.', 'nlp', 'is', 'a', 'key', 'applic', 'of', 'machin', 'learn', '.']\n",
      "Lancaster Stemmed Tokens: ['machin', 'learn', 'is', 'a', 'branch', 'of', 'art', 'intellig', '.', 'it', 'focus', 'on', 'build', 'system', 'that', 'learn', 'from', 'dat', '.', 'nlp', 'is', 'a', 'key', 'apply', 'of', 'machin', 'learn', '.']\n",
      "Snowball Stemmed Tokens: ['machin', 'learn', 'is', 'a', 'branch', 'of', 'artifici', 'intellig', '.', 'it', 'focus', 'on', 'build', 'system', 'that', 'learn', 'from', 'data', '.', 'nlp', 'is', 'a', 'key', 'applic', 'of', 'machin', 'learn', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "# Initialize the stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# Define a text to process\n",
    "text = \"Machine learning is a branch of artificial intelligence. It focuses on building systems that learn from data. NLP is a key application of machine learning.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Apply each stemmer\n",
    "porter_stems = [porter.stem(word) for word in tokens]\n",
    "lancaster_stems = [lancaster.stem(word) for word in tokens]\n",
    "snowball_stems = [snowball.stem(word) for word in tokens]\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Porter Stemmed Tokens:\", porter_stems)\n",
    "print(\"Lancaster Stemmed Tokens:\", lancaster_stems)\n",
    "print(\"Snowball Stemmed Tokens:\", snowball_stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 04: Lemmatization**\n",
    "\n",
    "#### **Understanding Lemmatization**\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or dictionary form, known as the lemma. Unlike stemming, lemmatization considers the context and the part of speech of the word, leading to more accurate reductions that are valid words.\n",
    "\n",
    "#### **Why Use Lemmatization?**\n",
    "\n",
    "1. Output Validity: Ensures that the base form of the word is a valid word.\n",
    "\n",
    "2. Context-Awareness: Considers the context and part of speech, leading to more accurate results.\n",
    "\n",
    "3. Improving Text Analysis: Useful for tasks that require understanding the meaning and context of the text.\n",
    "\n",
    "#### **Key Differences Between Stemming and Lemmatization**\n",
    "| Aspect               | Stemming                                                                                     | Lemmatization                                                                                     |\n",
    "|----------------------|----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n",
    "| Output Validity      | Produces base forms that may not be actual words (e.g., \"running\" -> \"run\", \"better\" -> \"bett\"). | Produces base forms that are valid words (e.g., \"running\" -> \"run\", \"better\" -> \"good\").           |\n",
    "| Context-Awareness    | Does not consider the context or part of speech of the word.                                  | Considers the context and part of speech, leading to more accurate results.                       |\n",
    "| Accuracy             | Can be too aggressive, leading to over-stemming (e.g., \"universities\" -> \"univers\").           | More precise, reducing words to their true base form.                                              |\n",
    "| Usage in NLP         | Useful for tasks where speed and simplicity are more important than accuracy (e.g., search engines). | Preferred for tasks requiring a deeper understanding of the text and more accurate results (e.g., sentiment analysis, text summarization). |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hitar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['The', 'children', 'are', 'playing', 'with', 'the', 'toys', '.', 'He', 'saw', 'the', 'cats', 'running', 'around', '.']\n",
      "Lemmatized Tokens: ['The', 'child', 'playing', 'toy', '.', 'He', 'saw', 'cat', 'running', 'around', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 01: Import necessary libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Step 02: Define a text to process\n",
    "text = \"The children are playing with the toys. He saw the cats running around.\"\n",
    "\n",
    "# Step 03: Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#Step 04: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_for_lemetization = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "# Step 05: Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step 06: Apply lemmatization to each token\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_for_lemetization]\n",
    "\n",
    "# Step 07: Display the tokens before and after lemmatization\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Here are some common types of algorithms and approaches used for lemmatization:**\n",
    "\n",
    "1. Dictionary-Based Lemmatization\n",
    "\n",
    "Dictionary-based lemmatization relies on predefined dictionaries or lexicons that map words to their base forms. These dictionaries contain information about the lemma of each word along with its part of speech (POS). Algorithms using this approach typically involve looking up each word in the dictionary and returning its lemma based on the specified POS.\n",
    "\n",
    "WordNet: A widely used lexical database for English. It provides a set of hierarchical synsets (groups of synonymous words) with short definitions and usage examples. Lemmatization with WordNet involves mapping words to their corresponding synsets and selecting the lemma based on context.\n",
    "\n",
    "2. Rule-Based Lemmatization\n",
    "\n",
    "Rule-based lemmatization utilizes linguistic rules and patterns to derive the lemma of a word. These rules are often language-specific and take into account morphological variations of words based on their part of speech and syntactic context.\n",
    "\n",
    "Morphological Rules: Rules are crafted based on the morphology of the language. For example, rules might specify how to handle plural forms, verb conjugations, and adjectival inflections.\n",
    "\n",
    "3. Stochastic Lemmatization\n",
    "\n",
    "Stochastic lemmatization employs statistical models and machine learning techniques to predict the lemma of a word based on large corpora of text. These models learn patterns and relationships between words and their lemmas from data, improving accuracy through statistical inference.\n",
    "\n",
    "Machine Learning Models: Techniques such as sequence models (e.g., Hidden Markov Models, Conditional Random Fields) or neural networks can be used to predict lemmas based on contextual features and linguistic patterns observed in training data.\n",
    "\n",
    "4. Hybrid Approaches\n",
    "\n",
    "Hybrid approaches combine multiple strategies, such as integrating dictionary-based methods with rule-based or statistical techniques. These approaches aim to leverage the strengths of each method to enhance accuracy and coverage in different linguistic contexts.\n",
    "\n",
    "Example Lemmatization Libraries and Tools\n",
    "NLTK (Natural Language Toolkit): Provides implementations of both dictionary-based (WordNet) and rule-based lemmatizers for various languages.\n",
    "\n",
    "SpaCy: A modern NLP library that supports lemmatization using statistical models and rule-based approaches for multiple languages.\n",
    "\n",
    "Stanford CoreNLP: A suite of NLP tools that includes lemmatization capabilities based on linguistic rules and statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hitar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hitar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')  # Download the POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized with WordNet: ['The', 'child', 'are', 'playing', 'with', 'the', 'toy', '.', 'He', 'saw', 'the', 'cat', 'running', 'around', '.']\n",
      "Lemmatized with NLTK Rules: ['The', 'child', 'be', 'play', 'with', 'the', 'toy', '.', 'He', 'saw', 'the', 'cat', 'run', 'around', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Function to perform lemmatization using WordNet\n",
    "def lemmatize_with_wordnet(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Function to perform lemmatization using NLTK's built-in POS tagging and rules\n",
    "def lemmatize_with_nltk_rules(text):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag is None:\n",
    "            lemmatized_tokens.append(word)  # fallback to original word if no suitable tag found\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(word, pos=wn_tag))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Helper function to map NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Sample text\n",
    "sample_text = \"The children are playing with the toys. He saw the cats running around.\"\n",
    "\n",
    "# Perform lemmatization using WordNet\n",
    "lemmatized_wordnet = lemmatize_with_wordnet(sample_text)\n",
    "print(\"Lemmatized with WordNet:\", lemmatized_wordnet)\n",
    "\n",
    "# Perform lemmatization using NLTK's built-in rules and POS tagging\n",
    "lemmatized_nltk_rules = lemmatize_with_nltk_rules(sample_text)\n",
    "print(\"Lemmatized with NLTK Rules:\", lemmatized_nltk_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "Lemmatized Tokens with POS: ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmetization along with POS tagging.\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Function to get POS tag in a format compatible with WordNetLemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Apply lemmatization with POS tagging\n",
    "lemmatized_tokens_pos = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "\n",
    "# Display the tokens before and after lemmatization\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Lemmatized Tokens with POS:\", lemmatized_tokens_pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summing up all the things we have learnt:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 01: Tokenized text ['Machine', 'learning', 'is', 'a', 'sub-branch', 'of', 'Artificial', 'Intelligence', '.', 'NLP', 'is', 'indid', 'a', 'sub-branch', 'of', 'Machine', 'Learning']\n",
      "\n",
      "Step 02: Removed stopwords ['Machine', 'learning', 'sub-branch', 'Artificial', 'Intelligence', '.', 'NLP', 'indid', 'sub-branch', 'Machine', 'Learning']\n",
      "\n",
      "Step 03: Stemmed words ['machin', 'learn', 'sub-branch', 'artifici', 'intellig', '.', 'nlp', 'indid', 'sub-branch', 'machin', 'learn']\n",
      "\n",
      "Step 04: Lemmatized words ['machin', 'learn', 'sub-branch', 'artifici', 'intellig', '.', 'nlp', 'indid', 'sub-branch', 'machin', 'learn']\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Take text\n",
    "\n",
    "text = input(\"Enter text: \")\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Step 01: Tokenized text\",tokens)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in tokens if not w in stop_words]\n",
    "print(\"\\nStep 02: Removed stopwords\",words)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "words = [stemmer.stem(w) for w in words]\n",
    "print(\"\\nStep 03: Stemmed words\",words)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [lemmatizer.lemmatize(w) for w in words]\n",
    "print(\"\\nStep 04: Lemmatized words\",words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
